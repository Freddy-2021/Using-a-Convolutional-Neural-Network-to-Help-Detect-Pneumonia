{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e573ea9f",
   "metadata": {},
   "source": [
    "![example](images/pexels-pixabay-40568.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39584590",
   "metadata": {},
   "source": [
    "# Phase 4 Project\n",
    "\n",
    "**Author:** Freddy Abrahamson<br>\n",
    "**Date created:** 7-1-2022<br>\n",
    "**Discipline:** Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa21282",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f6a7d",
   "metadata": {},
   "source": [
    "### In order for the notebook to run successfully:\n",
    "\n",
    "1. Download the data from https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "2. The download should be unzipped to the same folder as where your Jupyter notebook is located. The unzipped folder is called 'chest_xray'.\n",
    "\n",
    "\n",
    "### Business Problem\n",
    "\n",
    "In the United States about 1.5 million people were diagnosed with pneumonia in an emergency room, in 2018, and around 40,000 people died of pneumonia in that same year. Pneumonia is the worldâ€™s leading cause of death among children under 5 years of age, killing approximately 2,400 children a day in 2015. These facts underlie the importance, of not only a correct diagnosis, but a timely one. Now, with the introduction of COVID-19, and it's role as a precursor to pneumonia, the need for a cost effective, non-invasive diagnostic tool is even greater. A model that could correctly identify an x-ray as one of someone with pneumonia, would serve as a cost effective non-invasive tool, that could be used to prioritize which patients the doctor should see first.\n",
    "\n",
    "\n",
    "https://www.cdc.gov/dotw/pneumonia/index.html <br>\n",
    "https://www.thoracic.org/patients/patient-resources/resources/top-pneumonia-facts.pdf\n",
    "\n",
    "### Data Understanding and Data preparation\n",
    "\n",
    "The data was taken from Kaggle.com. There are a total of 5856 images. This includes 1583 'normal' images, and 4273 'pneumonia' images. The ratio of 'pneumonia' images to 'normal' images is about 2.7 : 1. I divided all these images between train , test, and val folders at a ratio of .8:.1:.1 respectively. I maintained the 2.7 to 1 ratio between the 'pneumonia' images and 'normal' images, for all the folders. Once how many of each image would go to each folder was established, all the 'normal', and 'pneumonia' images were chosen randomly. The primary concern with the dataset preparation would be to normalize the image values. All the values were scaled to a range between 0 and 1. \n",
    "\n",
    "### Modeling\n",
    "\n",
    "I used Keras and Tensorflow to create the models. Given that with the use of the filters, cnn(s) excel at detecting features of various sizes,I chose to use the less apt multi-layer perceptron as a baseline model. I then tried to overfit on purpose using a cnn. I began with a cnn model that has 4 activation layers for the feature extraction part,with the number of nodes for each each layer being 16,32,64, and 128 respectively. I used ReLu as my activation function for all feature detection, as well as for the classification layers. Given that this is a binary classification problem (0 for normal, and 1 for pneumonia), I used a sigmoid function for the output layer. From there, based on the results, I would either try to reduce the bias, by adding a layer, adding more nodes to existing layers, or both; or reduce the variance by increasing the filter size to improve generalizability, or add dropout layers.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Given the importance of correctly identifying a patient with pneumonia, my primary goal was to find a model that produced the best recall scores. To this end, I was looking for a model that would produce the best bias/variance combination between the train and test data sets. I did this by creating a function best_model(), which utilizes the auc() function from sklearn.metrics. The x-axis is represented by the absolute difference between the train and test scores, while the y-axis is represented by the test scores. The higher the test score, and the lower the train-test difference, the greater the area under the curve. The function returns a dataframe with the models, and their respective test scores, sorted by their auc. The model with the highest auc is the best. The secondary goal was a model that would have a good accuracy score, which the 'best' model in fact does, with a score over 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5002276",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import models, layers, optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1559f",
   "metadata": {},
   "source": [
    "Creating random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c85e2",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4106e62",
   "metadata": {},
   "source": [
    "## best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265113b8",
   "metadata": {},
   "source": [
    "The best_model function returns the best train test score combination based on the auc function, where\n",
    "the difference between the test and the train scores represents the x axis, and test score represents\n",
    "the y axis. In order use the auc function, for each x,y coordinate we created a list of length three,\n",
    "with 0 and 1 at the ends, and the actual x,y values in the middle. The model with the highest auc score is the best. \n",
    "\n",
    "This function takes as as an argument an integer which represents the total number of models.\n",
    "\n",
    "\n",
    "This function returns a dataframe with five columns:\n",
    "  1. The model name\n",
    "  2. The recall score for the train set\n",
    "  3. The recall score for the test set\n",
    "  4. The absolute value of the difference between the two scores\n",
    "  5. The auc score, sorted in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(n):\n",
    "    from sklearn.metrics import auc\n",
    "    scores_df = neural_network_model_scores_df(n)\n",
    "\n",
    "#   creating 'test_scores' and 'score_diffs' zero populated lists of shape(rows,3) \n",
    "    rows = len(scores_df)\n",
    "    test_scores = np.zeros((rows, 3))\n",
    "    score_diffs = np.zeros((rows, 3))\n",
    "    auc_scores = []\n",
    "\n",
    "#   populating 'test_scores' and 'score_diffs' so each list has a format [0,test_score,1],\n",
    "#   and [0,score_diff,1] respectively\n",
    "    for row in range(rows):\n",
    "        test_scores[row][1] = scores_df['test score'][row]\n",
    "        test_scores[row][2] = 1\n",
    "        score_diffs[row][1] = scores_df['train-test diff'][row]\n",
    "        score_diffs[row][2] = 1\n",
    "\n",
    "#   creating a list of all the auc scores\n",
    "    for row in range(rows):\n",
    "        auc_score = auc(score_diffs[row], test_scores[row])\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "#   getting the greatest auc score, and the index number of that row    \n",
    "    best_auc_score = max(auc_scores)\n",
    "    best_score_index = auc_scores.index(best_auc_score)\n",
    "    \n",
    "#   add an auc_score cloumn to scores_df\n",
    "    scores_df['auc score'] = auc_scores\n",
    "    \n",
    "\n",
    "#   return scores_df sprted by auc score\n",
    "    return scores_df.sort_values(by='auc score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc51b38",
   "metadata": {},
   "source": [
    "## neural_network_model_scores_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b5499",
   "metadata": {},
   "source": [
    "This function takes as as an argument an integer which represents the total number of models.\n",
    "\n",
    "This function returns a dataframe with four columns:\n",
    "   1. The model name\n",
    "   2. The recall score for the train set\n",
    "   3. The recall score for the test set\n",
    "   4. The absolute value of the difference between the two scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model_scores_df (n):\n",
    "    count = 0\n",
    "    model_dict ={}\n",
    "    model_dict_list =[]\n",
    "    while count<n:\n",
    "        if count == 0:\n",
    "            train_score = globals()['baseline_train_eval_dict']['recall']\n",
    "            test_score = globals()['baseline_test_eval_dict']['recall']\n",
    "            model_name = 'baseline_model'\n",
    "            model_dict = {'model name':model_name, 'train score':train_score, 'test score':test_score}\n",
    "            model_dict_list.append(model_dict)\n",
    "            count+=1\n",
    "        else:\n",
    "            train_score = globals()['cnn_' +str(count) +'_train_eval_dict']['recall']\n",
    "            test_score = globals()['cnn_' +str(count) +'_test_eval_dict']['recall']\n",
    "            model_name = 'cnn_model_'+ str(count)\n",
    "            model_dict = {'model name':model_name, 'train score':train_score, 'test score':test_score}\n",
    "            model_dict_list.append(model_dict)\n",
    "            count+=1\n",
    "    scores_df = pd.DataFrame(model_dict_list)\n",
    "    scores_df['train-test diff'] = abs(scores_df['train score'] - scores_df['test score'])\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba48371",
   "metadata": {},
   "source": [
    "## recall_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700209c",
   "metadata": {},
   "source": [
    "This function takes as an as an argument a dictionary returned by the evaluate() method from a tensorflow model, that includes recall, among it's metrics.\n",
    "\n",
    "This function returns the following:\n",
    "   1. The key/value pair corresponding to the recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72354b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_dict(eval_dict):\n",
    "    eval_dict = {key: eval_dict[key] for key in eval_dict.keys()\n",
    "                 & {'recall'}}\n",
    "    for key in eval_dict.keys():\n",
    "        eval_dict[key] = round(eval_dict[key],4)\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfc03e",
   "metadata": {},
   "source": [
    "## plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5512ae20",
   "metadata": {},
   "source": [
    "Note: The code for this function was taken from the instructional website: https://deeplizard.com/learn/video/km7pxKy4UHU\n",
    "\n",
    "This function takes the following arguments:\n",
    "   1. cm: The array representing the results from the scikit-learn confusion_matrix() function\n",
    "   2. classes: an array with the name of the class labels of the confusion matrix.\n",
    "   3. normalize: if True, normalizes the values displayed by the confusion matrix. Default is False.\n",
    "   4. title: The title of the confusion matrix. Default is 'Confusion Matrix'.\n",
    "   5. cmap: Matplotlib colormap. Default is plt.cm.Blues.\n",
    "\n",
    "This function returns a graphical plot of the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e94fd5",
   "metadata": {},
   "source": [
    "## visualize_training_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684baae",
   "metadata": {},
   "source": [
    "This function takes as an as an argument the variable returned by fitting a tensorflow model. \n",
    "it is of type: tensorflow.python.keras.callbacks.History\n",
    "\n",
    "\n",
    "The best_model function returns the following plot:\n",
    "1. The change in recall(y-axis) with respect to the number of epochs(x-axis)for both the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(results):\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['recall'])\n",
    "    plt.plot(history['val_recall'])\n",
    "    plt.legend(['recall', 'val_recall'])\n",
    "    plt.title('Train Recall and Validation Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Train Recall and Val Recall')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5a231",
   "metadata": {},
   "source": [
    "# Importing and Organizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cdbf43",
   "metadata": {},
   "source": [
    "I will create two new folders: 'all_normal' and 'all_pneumonia', and copy all the corresponding images from the 'chest_xray' folder to these folders. I will then create a folder called 'train_test_val', with three folders inside of it:'train', 'test', and 'val'. Each of these three folders will have a 'normal', and a 'pneumonia' folder. I will randomly copy the images from the 'all_normal', and 'all_pneumonia' folders, to these three folders, with a split of 80%,10%,10% respectively, keeping the ratio between the number of 'normal' and 'pneumonia' images uniform across all three folders (stratified). The ratio of pneumonia images to normal images is 2.7:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7187f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 'all_normal' and 'all_pneumonia' folders\n",
    "all_normal_dir = 'all_normal/'\n",
    "all_pneumonia_dir = 'all_pneumonia/'\n",
    "os.mkdir(all_normal_dir)\n",
    "os.mkdir(all_pneumonia_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81435fd7",
   "metadata": {},
   "source": [
    "Creating the 'train_test_val' folder and all of its subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train_test_val folders:\n",
    "\n",
    "# train_test_val parent folder:\n",
    "train_test_val_dir = 'train_test_val/'\n",
    "os.mkdir(train_test_val_dir)\n",
    "\n",
    "# train folders:\n",
    "train_dir = 'train_test_val/train'\n",
    "train_normal_dir = 'train_test_val/train/normal'\n",
    "train_pneumonia_dir = 'train_test_val/train/pneumonia'\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(train_normal_dir)\n",
    "os.mkdir(train_pneumonia_dir)\n",
    "\n",
    "# test folders:\n",
    "test_dir = 'train_test_val/test'\n",
    "test_normal_dir = 'train_test_val/test/normal'\n",
    "test_pneumonia_dir = 'train_test_val/test/pneumonia'\n",
    "os.mkdir(test_dir)\n",
    "os.mkdir(test_normal_dir)\n",
    "os.mkdir(test_pneumonia_dir)\n",
    "\n",
    "# val folders:\n",
    "val_dir = 'train_test_val/val'\n",
    "val_normal_dir = 'train_test_val/val/normal'\n",
    "val_pneumonia_dir = 'train_test_val/val/pneumonia'\n",
    "os.mkdir(val_dir)\n",
    "os.mkdir(val_normal_dir)\n",
    "os.mkdir(val_pneumonia_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d156112",
   "metadata": {},
   "source": [
    "Creating lists with name of the files that are in the 'pneumonia' and 'normal' folders contained in the 'chest_xray'folder. There are there sets of 'pneumonia' and 'normal' folders. One for each of the train, \n",
    "test and val folders. There are 6 lists total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with the file names for the respective original 'NORMAL' folders\n",
    "normal_train_imgs = [file for file in os.listdir('chest_xray/train/NORMAL') if file.endswith('.jpeg')]\n",
    "normal_test_imgs = [file for file in os.listdir('chest_xray/test/NORMAL') if file.endswith('.jpeg')]\n",
    "normal_val_imgs = [file for file in os.listdir('chest_xray/val/NORMAL') if file.endswith('.jpeg')]\n",
    "\n",
    "# creating a list with the file names for the respective original 'PNEUMONIA' folders\n",
    "pneumonia_train_imgs = [file for file in os.listdir('chest_xray/train/PNEUMONIA') if file.endswith('.jpeg')]\n",
    "pneumonia_test_imgs = [file for file in os.listdir('chest_xray/test/PNEUMONIA') if file.endswith('.jpeg')]\n",
    "pneumonia_val_imgs = [file for file in os.listdir('chest_xray/val/PNEUMONIA') if file.endswith('.jpeg')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b88dda",
   "metadata": {},
   "source": [
    "I will use the lists to copy all the files to their corresponding folder, either 'all_normal' or 'all_pneumonia'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying all the images from the original normal train folder to 'all_normal' folder\n",
    "for img in normal_train_imgs:\n",
    "    origin = os.path.join('chest_xray/train/NORMAL', img)\n",
    "    destination = os.path.join('all_normal/', img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying all the images from the original normal test folder to 'all_normal' folder\n",
    "for img in normal_test_imgs:\n",
    "    origin = os.path.join('chest_xray/test/NORMAL', img)\n",
    "    destination = os.path.join('all_normal/', img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying all the images from the original normal val folder to 'all_normal' folder\n",
    "for img in normal_val_imgs:\n",
    "    origin = os.path.join('chest_xray/val/NORMAL', img)\n",
    "    destination = os.path.join('all_normal/', img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying all the images from the original pneumonia train folder to 'all_pneumonia' folder\n",
    "for img in pneumonia_train_imgs:\n",
    "    origin = os.path.join('chest_xray/train/PNEUMONIA', img)\n",
    "    destination = os.path.join('all_pneumonia/', img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying all the images from the original pneumonia test folder to 'all_pneumonia' folder\n",
    "for img in pneumonia_test_imgs:\n",
    "    origin = os.path.join('chest_xray/test/PNEUMONIA', img)\n",
    "    destination = os.path.join('all_pneumonia/', img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying all the images from the original pneumonia val folder to 'all_pneumonia' folder\n",
    "for img in pneumonia_val_imgs:\n",
    "    origin = os.path.join('chest_xray/val/PNEUMONIA', img)\n",
    "    destination = os.path.join('all_pneumonia/', img)\n",
    "    shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf958cd",
   "metadata": {},
   "source": [
    "Calculating image totals, and the ratio of 'pneumonia' to 'normal' folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_normal_list = os.listdir('all_normal/')\n",
    "all_pneumonia_list = os.listdir('all_pneumonia/')\n",
    "print('There are ',len(all_normal_list),' total normal images')\n",
    "print('There are ',len(all_pneumonia_list),' total pneumonia images')\n",
    "print('There are ',len(all_normal_list) + len(all_pneumonia_list),' total images')\n",
    "ratio = (round(len(all_pneumonia_list)/len(all_normal_list),2))\n",
    "print('The ratio of pneumonia images to normal images is aproximately ', ratio,':1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283cb05",
   "metadata": {},
   "source": [
    "Calculating how many of each file will go the corresponding 'train', 'test', and 'val' folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''The train/test/val split will be approximately .8,.1,.1, with an approximate 2.7:1 ratio \n",
    "between the pneumonia and normal images respectively.\\n''')\n",
    "\n",
    "print('The total number of validation images is: ',round(5856*.1))\n",
    "print('The total number of normal validation images is: ',round(586/3.7))\n",
    "print('The total number of pneumonia validation images is: ',round(586-158),'\\n')\n",
    "\n",
    "print('The total number of test images is: ',round(5856*.1))\n",
    "print('The total number of normal test images is: ',round(586/3.7))\n",
    "print('The total number of pneumonia test images is: ',round(586-158),'\\n')\n",
    "\n",
    "print('The total number of train images is: ',(5856 - (2*586)))\n",
    "print('The total number of normal train images is: ',round(4684/3.7)+1)\n",
    "print('The total number of pneumonia train images is: ',round(4684-1267),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661c36e",
   "metadata": {},
   "source": [
    "Creating lists that will be used to copythe files from the 'all_normal' and 'all_pneumonia' folders to the 'train', 'test', and'val' folders. The file names that are added to the list are chosen randomly without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "val_normal_list = random.sample(all_normal_list, 158)\n",
    "for img in val_normal_list:\n",
    "    all_normal_list.remove(img)\n",
    "print('The len of all_normal_imgs_list of removing val_normal_list images is: ', len(all_normal_list))    \n",
    "\n",
    "random.seed(2)    \n",
    "test_normal_list = random.sample(all_normal_list, 158)\n",
    "for img in test_normal_list:\n",
    "    all_normal_list.remove(img)\n",
    "print('The len of all_normal_imgs_list of removing test_normal_list images is: ', len(all_normal_list))    \n",
    "\n",
    "random.seed(3)\n",
    "train_normal_list = random.sample(all_normal_list, 1267)\n",
    "for img in train_normal_list:\n",
    "    all_normal_list.remove(img)\n",
    "print('The len of all_normal_imgs_list of removing train_normal_list images is: ', len(all_normal_list))    \n",
    "\n",
    "random.seed(4)\n",
    "val_pneumonia_list = random.sample(all_pneumonia_list, 428)\n",
    "for img in val_pneumonia_list:\n",
    "    all_pneumonia_list.remove(img)\n",
    "print('The len of all_pneumonia_imgs_list after removing val_pneumonia_list images is:',len(all_pneumonia_list))    \n",
    "\n",
    "random.seed(5)    \n",
    "test_pneumonia_list = random.sample(all_pneumonia_list, 428)\n",
    "for img in test_pneumonia_list:\n",
    "    all_pneumonia_list.remove(img)\n",
    "print('The len of all_pneumonia_imgs_list after removing test_pneumonia_list images is: ', len(all_pneumonia_list))    \n",
    "\n",
    "random.seed(6)\n",
    "train_pneumonia_list = random.sample(all_pneumonia_list, 3417)\n",
    "for img in train_pneumonia_list:\n",
    "    all_pneumonia_list.remove(img)\n",
    "print('The len of all_pneumonia_imgs_list after removing train_pneumonia_list images is: ', len(all_pneumonia_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03deb65",
   "metadata": {},
   "source": [
    "I will use the lists to copy all the files to their corresponding folder, either 'train','test', or 'val'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfded38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying 'all_normal' images to normal val folder \n",
    "for img in val_normal_list:\n",
    "    origin = os.path.join(all_normal_dir, img)\n",
    "    destination = os.path.join(val_normal_dir, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying 'all_normal' images to normal test folder \n",
    "for img in test_normal_list:\n",
    "    origin = os.path.join(all_normal_dir, img)\n",
    "    destination = os.path.join(test_normal_dir, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying 'all_normal' images to normal train folder \n",
    "for img in train_normal_list:\n",
    "    origin = os.path.join(all_normal_dir, img)\n",
    "    destination = os.path.join(train_normal_dir, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying 'pneumonia' images to pneumonia val folder \n",
    "for img in val_pneumonia_list:\n",
    "    origin = os.path.join(all_pneumonia_dir, img)\n",
    "    destination = os.path.join(val_pneumonia_dir, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying 'normal' images to pneumonia test folder \n",
    "for img in test_pneumonia_list:\n",
    "    origin = os.path.join(all_pneumonia_dir, img)\n",
    "    destination = os.path.join(test_pneumonia_dir, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# copying 'normal' images to pneumonia train folder \n",
    "for img in train_pneumonia_list:\n",
    "    origin = os.path.join(all_pneumonia_dir, img)\n",
    "    destination = os.path.join(train_pneumonia_dir, img)\n",
    "    shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1c26a",
   "metadata": {},
   "source": [
    "Confirming all the folders have the correct number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2848b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check to make sure all the folders have the correct number of files:\n",
    "\n",
    "print('The val normal folder has ',len(os.listdir(val_normal_dir)))\n",
    "print('The val pneumonia folder has ',len(os.listdir(val_pneumonia_dir)))\n",
    "print('The test normal folder has ',len(os.listdir(test_normal_dir)))\n",
    "print('The test pneumonia folder has ',len(os.listdir(test_pneumonia_dir)))\n",
    "print('The train normal folder has ',len(os.listdir(train_normal_dir)))\n",
    "print('The train pneumonia folder has ',len(os.listdir(train_pneumonia_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80661781",
   "metadata": {},
   "source": [
    "Creating instances of generators that will serve two purposes:\n",
    "    1. Scale all the image values to a value between 0 and 1.\n",
    "    2. Transfer the images from the 'train', 'test', and 'val' folders to their corresponding numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the data in the directory train_test_val/val (586 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_dir, \n",
    "        target_size=(64, 64), batch_size = 586)\n",
    "\n",
    "# get all the data in the directory train_test_val/test (586 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_dir, \n",
    "        target_size=(64, 64), batch_size = 586) \n",
    "\n",
    "# get all the data in the directory train_test_val/train (4684 images), and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_dir, \n",
    "        target_size=(64, 64),batch_size = 4684)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3162b9",
   "metadata": {},
   "source": [
    "Using the generators to copy the images to their corresponding data sets. For each group 'train', 'test'and 'val', there is an array of images, and an array of corresponding image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb696dd1",
   "metadata": {},
   "source": [
    "Reviewing the shapes of the datasets that were created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset again\n",
    "train_img_count = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "test_img_count = test_images.shape[0]\n",
    "val_img_count = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(train_img_count))\n",
    "print (\"Number of testing samples: \" + str(test_img_count))\n",
    "print (\"Number of validation samples: \" + str(val_img_count))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535dfe6",
   "metadata": {},
   "source": [
    "Creating three new datasets, one each for 'train', 'test', and 'val' images, where the arrays are unrowed. This is required for use with the multi-layer perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "train_img_unrow_dataset = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img_unrow_dataset = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img_unrow_dataset = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img_unrow_dataset.shape)\n",
    "print(test_img_unrow_dataset.shape)\n",
    "print(val_img_unrow_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97ba4c",
   "metadata": {},
   "source": [
    "Repeating the process for all the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a11a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unrow_img_labels = np.reshape(train_labels[:,0], (4684,1))\n",
    "test_unrow_img_labels = np.reshape(test_labels[:,0], (586,1))\n",
    "val_unrow_img_labels = np.reshape(val_labels[:,0], (586,1))\n",
    "\n",
    "print(train_unrow_img_labels.shape)\n",
    "print(test_unrow_img_labels.shape)\n",
    "print(val_unrow_img_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34afe634",
   "metadata": {},
   "source": [
    "# Reveiwing the Data Before Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48dafab",
   "metadata": {},
   "source": [
    "## Checking If Data Needs to be Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize pixel values\n",
    "print('Train images min value:', train_images.min(),'Train images max value:', train_images.max())\n",
    "print('Validation images min value:', val_images.min(),'Validation images max value:', val_images.max())\n",
    "print('Test images min value:', test_images.min(),'Test images max value:', test_images.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbaefe9",
   "metadata": {},
   "source": [
    "The images are already normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891a8b2",
   "metadata": {},
   "source": [
    "## Viewing an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9320d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view an image of 'normal' xray\n",
    "normal_img = load_img('all_normal/IM-0001-0001.jpeg', target_size=(256, 256))\n",
    "print('normal image')\n",
    "normal_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82801af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an image of 'normal' xray\n",
    "pneumonia_img = load_img('all_pneumonia/person1_bacteria_1.jpeg', target_size=(256, 256))\n",
    "print('pneumonia image')\n",
    "pneumonia_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35630fc",
   "metadata": {},
   "source": [
    "# Neural Network models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207edfd6",
   "metadata": {},
   "source": [
    "## Building a multi-layer perceptron as a baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d53602",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "# Build a baseline fully connected model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "baseline_model = models.Sequential()\n",
    "baseline_model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) \n",
    "baseline_model.add(layers.Dense(7, activation='relu'))\n",
    "baseline_model.add(layers.Dense(5, activation='relu'))\n",
    "baseline_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "baseline_model.compile(optimizer='sgd',\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c95447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_baseline =  baseline_model.fit(train_img_unrow_dataset,\n",
    "                    train_unrow_img_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_img_unrow_dataset,val_unrow_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517930c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5641c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_train_eval_dict = baseline_model.evaluate(train_img_unrow_dataset, train_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(baseline_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90ddc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_val_eval_dict = baseline_model.evaluate(val_img_unrow_dataset, val_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(baseline_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aadb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test_eval_dict = baseline_model.evaluate(test_img_unrow_dataset, test_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(baseline_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5b394",
   "metadata": {},
   "source": [
    "The recall scores are actually very good, with a test score of .9051 and a difference between the train and test scores of about only 2.5. I will try to improve this using a cnn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5a13e",
   "metadata": {},
   "source": [
    " ## Build a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0fd58",
   "metadata": {},
   "source": [
    " ### CNN-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6d4f8",
   "metadata": {},
   "source": [
    "The goal here is to overfit; try to beat the ann train score of 0.9313, and try to reduce variance from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_1 = models.Sequential()\n",
    "cnn_model_1.add(layers.Conv2D(16, (2, 2), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "cnn_model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_1.add(layers.Flatten())\n",
    "cnn_model_1.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model_1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model_1.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e18540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn1 = cnn_model_1.fit(train_images,\n",
    "                    train_unrow_img_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_unrow_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b76ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1_train_eval_dict = cnn_model_1.evaluate(train_images, train_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_1_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1_val_eval_dict = cnn_model_1.evaluate(val_images, val_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_1_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1_test_eval_dict = cnn_model_1.evaluate(test_images, test_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_1_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f471a5",
   "metadata": {},
   "source": [
    "The train score is about the same as that of the baseline. My main concern right now is trying to reduce potetntial bias so I will add more nodes to the next model and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba22805",
   "metadata": {},
   "source": [
    " ### CNN-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_2 = models.Sequential()\n",
    "cnn_model_2.add(layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "cnn_model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_2.add(layers.Flatten())\n",
    "cnn_model_2.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model_2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model_2.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618aa99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn2 = cnn_model_2.fit(train_images,\n",
    "                    train_unrow_img_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_unrow_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11703eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    " visualize_training_results(results_cnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ad42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2_train_eval_dict = cnn_model_2.evaluate(train_images, train_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_2_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2_val_eval_dict = cnn_model_2.evaluate(val_images, val_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_2_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d320817",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn_2_test_eval_dict = cnn_model_2.evaluate(test_images, test_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_2_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d866ee6",
   "metadata": {},
   "source": [
    "After adding 16 nodes to the second activation layer, the train score is virtually the same, with the recall score actually going back up relative to 'cnn1'. 93% is a pretty high score, and I am not certain I can improve on that, so I will try to reduce variance with the next model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20ee9c",
   "metadata": {},
   "source": [
    "### CNN-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770b0bc",
   "metadata": {},
   "source": [
    "I increased the filter size in the fourth activation layer, in an attempt to increase generalizability, and therefor reduce variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_3 = models.Sequential()\n",
    "cnn_model_3.add(layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "cnn_model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_3.add(layers.Flatten())\n",
    "cnn_model_3.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model_3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model_3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a3e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn3 = cnn_model_3.fit(train_images,\n",
    "                    train_unrow_img_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_unrow_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70001495",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9df364",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_train_eval_dict = cnn_model_3.evaluate(train_images, train_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_3_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_val_eval_dict = cnn_model_3.evaluate(val_images, val_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_3_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c04f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_test_eval_dict = cnn_model_3.evaluate(test_images, test_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_3_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40715daf",
   "metadata": {},
   "source": [
    "Interestingly enough, this not only gave me the highest train score so far, but the highest test score as well. This leads me to belive we can still work on reducing the bias. I will keep the 3x3 filter size for the fourth activation layer on the next model, but I will increase the number of nodes on the third activation layer from 64 to 96."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425b99f",
   "metadata": {},
   "source": [
    " ### CNN-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7db61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_4 = models.Sequential()\n",
    "cnn_model_4.add(layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Conv2D(96, (2, 2), activation='relu'))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "cnn_model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_4.add(layers.Flatten())\n",
    "cnn_model_4.add(layers.Dense(32, activation='relu'))\n",
    "cnn_model_4.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model_4.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7bb1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn4 = cnn_model_4.fit(train_images,\n",
    "                    train_unrow_img_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_unrow_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb856c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_4_train_eval_dict = cnn_model_4.evaluate(train_images, train_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_4_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb328be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(cnn_4_train_eval_dict['recall'],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c7302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_4_val_eval_dict = cnn_model_4.evaluate(val_images, val_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_4_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47888fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_4_test_eval_dict = cnn_model_4.evaluate(test_images, test_unrow_img_labels, return_dict=1, verbose=0)\n",
    "recall_dict(cnn_4_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300dfd3",
   "metadata": {},
   "source": [
    "These are the best scores so far. Both the train and test scores have risen about 1.5 points, with the train scoreat over 96%. I am satisfied with this score. I will try to reduce variance on the next model by introducing a small regularization adjustment, by applying a dropout layer of .1 to the fourth activation layer (128 nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be159d7",
   "metadata": {},
   "source": [
    " ### CNN-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "cnn_model_5 = models.Sequential()\n",
    "cnn_model_5.add(layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "cnn_model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_5.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "cnn_model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_5.add(layers.Conv2D(96, (2, 2), activation='relu'))\n",
    "cnn_model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_5.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "cnn_model_5.add(layers.Dropout(0.1))\n",
    "cnn_model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model_5.add(layers.Flatten())\n",
    "cnn_model_5.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model_5.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model_5.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd1b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cnn5 = cnn_model_5.fit(train_images,\n",
    "                    train_unrow_img_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    verbose=2,\n",
    "                    validation_data=(val_images, val_unrow_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4047ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(results_cnn5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5d20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_5_train_eval_dict = cnn_model_5.evaluate(train_images, train_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_5_train_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bddcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_5_val_eval_dict = cnn_model_5.evaluate(val_images, val_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_5_val_eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f99917",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_5_test_eval_dict = cnn_model_5.evaluate(test_images, test_unrow_img_labels,return_dict=1, verbose=0)\n",
    "recall_dict(cnn_5_test_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b7463f",
   "metadata": {},
   "source": [
    "This model returned the best scores, making a small improvement on the train score, and increasing by aproximately 1.5 points, relative to 'CNN-4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd960956",
   "metadata": {},
   "source": [
    "# Choosing Best Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5c662",
   "metadata": {},
   "source": [
    "The best_model() function returns a dataframe sorted by auc score. The model with the highest auc score is the model with the best bias-variance balance, and therefore the best model. In this case the best model is cnn_model_5, with a test score of 0.943038, and a train-test difference of 0.023813. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9973fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df_sorted = best_model(6)\n",
    "scores_df_sorted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29628029",
   "metadata": {},
   "source": [
    "# Best Model Confusion Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28227c",
   "metadata": {},
   "source": [
    "I created a confusion matrix plot for the presentation, as well as an easy way to calculate the accuracy score. With 559 0ut of 586 images correctly classified, this model has an accuracy score of over 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818a223",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "# creating the list of predictions and rounding to 0 or 1\n",
    "cnn_model_5_preds = cnn_model_5.predict(test_images)\n",
    "best_model_rounded_preds  = np.round(cnn_model_5_preds)\n",
    "\n",
    "#The following are the arguments required to create a visual plot of the confusion matrix:\n",
    "# scikit-learn confusion matrix returns a numerical array with: tp, fp, tn, and fn  \n",
    "cm = confusion_matrix(y_true=test_unrow_img_labels, y_pred=best_model_rounded_preds)\n",
    "# list with the plot labels (required as an argument)\n",
    "cm_plot_labels = ['pneumonia','normal']\n",
    "\n",
    "\n",
    "# plotting confusion matrix:\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff850d41",
   "metadata": {},
   "source": [
    "# Project Conclusion: Possible Further Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38727a",
   "metadata": {},
   "source": [
    "1. Run the model on a larger dataset\n",
    "2. Run the model on a balanced datset\n",
    "3. Use SMOTE or an image generator to attempt to balance the training data\n",
    "4. Create a custom metric that would take into account both precision and accuracy, but give priority to precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
